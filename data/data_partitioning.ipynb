{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_path='./data/ProcessedFile/Prism_secondary_auc.csv'\n",
    "drug_info_path = \"./data/ProcessedFile/sec_auc_drug.csv\"\n",
    "cell_info_path = \"./data/ProcessedFile/22Q1expressions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random partitioning repeated 5 times for random testing, unseen drug testing, and unseen cell line testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "seed_list = np.random.randint(1,100000,size = 5)\n",
    "save_path=f'./data/ProcessedFile/data_split'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "dataset = response\n",
    "split = \"random\"  \n",
    "train_dict,val_dict,test_dict = {},{},{}\n",
    "for seed in seed_list:\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if split == 'cell_blind':\n",
    "        col = \"depmap_id\"\n",
    "        objs = np.random.permutation(dataset[col].unique())\n",
    "    elif split == 'drug_blind':\n",
    "        col = \"name\"\n",
    "        objs = np.random.permutation(dataset[col].unique())\n",
    "    elif split == 'random':\n",
    "        objs = np.random.permutation(len(dataset))\n",
    "    \n",
    "    train_nums = int(0.8*len(objs))\n",
    "    val_nums = int(0.1*len(objs))+1\n",
    "    test_nums = len(objs) - train_nums - val_nums  \n",
    "    \n",
    "    if split == 'cell_blind' or split == 'drug_blind':\n",
    "        targets=[]\n",
    "        other_objs  = [obj for obj in objs if obj not in targets]\n",
    "        train_objs = other_objs[:train_nums]\n",
    "        valid_objs = other_objs[train_nums:train_nums+val_nums]\n",
    "        test_objs = other_objs[train_nums+val_nums:]+targets\n",
    "        train = dataset.loc[dataset[col].isin(train_objs)]\n",
    "        val = dataset.loc[dataset[col].isin(valid_objs)]\n",
    "        test = dataset.loc[dataset[col].isin(test_objs)]\n",
    "    elif split == 'random':\n",
    "        train_index = objs[:train_nums]\n",
    "        val_index = objs[train_nums:train_nums+val_nums]\n",
    "        test_index = objs[train_nums+val_nums:]\n",
    "\n",
    "        train = dataset.iloc[train_index ,:]\n",
    "        val = dataset.iloc[val_index,:]\n",
    "        test = dataset.iloc[test_index,:]\n",
    "\n",
    "    train_dict[seed]=train\n",
    "    val_dict[seed]=val\n",
    "    test_dict[seed] = test\n",
    "\n",
    "    train.to_csv(os.path.join(save_path,f'{seed}{split}_Training.csv'),index=False)\n",
    "    val.to_csv(os.path.join(save_path,f'{seed}{split}_Validation.csv'),index=False)\n",
    "    test.to_csv(os.path.join(save_path,f'{seed}{split}_Test.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping drugs/cell lines by similarity and partitioning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning based on drug similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering based on drug similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morgan_fp(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "\n",
    "\n",
    "def calculate_tanimoto_distances(fps):\n",
    "    dist_matrix = []\n",
    "    N = len(fps)\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            dist = 1 - DataStructs.TanimotoSimilarity(fps[i], fps[j])  \n",
    "            dist_matrix.append(dist) \n",
    "    return dist_matrix\n",
    "\n",
    "def adjust_clusters(dist_matrix, num_samples,num_groups, threshold=0.3, random_seed=42):\n",
    "    clusters = list(Butina.ClusterData(dist_matrix , num_samples, distThresh=threshold, isDistData=True))\n",
    "    target_size = num_samples // num_groups\n",
    "    final_clusters = []\n",
    "\n",
    "    random.seed(random_seed)  \n",
    "    random.shuffle(clusters)\n",
    "\n",
    "    current_cluster = []\n",
    "    current_size = 0\n",
    "    for i, cluster in tqdm(enumerate(clusters)):\n",
    "        current_cluster.extend(cluster)  \n",
    "        current_size += len(cluster)  \n",
    "\n",
    "        if current_size >= target_size:\n",
    "            final_clusters.append(current_cluster)  \n",
    "            current_cluster = []  \n",
    "            current_size = 0 \n",
    "\n",
    "\n",
    "    if current_cluster:\n",
    "        final_clusters.append(current_cluster)  \n",
    "\n",
    "\n",
    "    while len(final_clusters) > num_groups:\n",
    "        small_cluster = final_clusters.pop()  \n",
    "        final_clusters[-1].extend(small_cluster)  \n",
    "\n",
    "\n",
    "    if len(final_clusters) < num_groups:\n",
    "        print(len(final_clusters),num_groups)\n",
    "        raise ValueError(f\"Cannot reach the target number of clusters ({num_groups}) with the current threshold. Please adjust the threshold or the target cluster count.\")\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(dataset_path)\n",
    "drug_info = pd.read_csv(drug_info_path)\n",
    "\n",
    "smiles_dict = dict(zip(drug_info.name, drug_info.canonical_smiles))\n",
    "smiles_list = list(smiles_dict.values())\n",
    "drug_ids = list(smiles_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_smiles = smiles_list\n",
    "\n",
    "fps = [get_morgan_fp(smiles) for smiles in drug_smiles]\n",
    "\n",
    "dist_matrix = calculate_tanimoto_distances(fps)\n",
    "\n",
    "num_groups = 10  \n",
    "threshold = 0.3 \n",
    "\n",
    "\n",
    "drug_random_seed = 42\n",
    "clusters = adjust_clusters(dist_matrix,len(drug_smiles), num_groups, threshold, random_seed=drug_random_seed)\n",
    "\n",
    "smiles_df = pd.DataFrame({'drug_id': drug_ids, 'smiles': smiles_list})\n",
    "\n",
    "for i,cluster in tqdm(enumerate(clusters)):\n",
    "    smiles_df.loc[cluster, 'group'] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_df.to_csv(f'./data/ProcessedFile/data_split/{drug_random_seed}_clustered_drug_groups.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset partitioning based on drug grouping information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_group_info = smiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_group_dict = {}\n",
    "for i in range(1,11):\n",
    "    cgroup_df = drug_group_info[drug_group_info['group']==i]\n",
    "    drug_group_dict[i]= cgroup_df['drug_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_name = \"name\"\n",
    "split_type=\"drug_sim_blind\"\n",
    "clusterway = drug_random_seed \n",
    "\n",
    "save_path = f'./data/ProcessedFile/data_split'\n",
    "for i in range(1,11):\n",
    "    testset = data_df[data_df[select_name].isin(drug_group_dict[i])].copy(True)\n",
    "    val_index = i+1 if i<10 else 1  \n",
    "    valset = data_df[data_df[select_name].isin(drug_group_dict[val_index])].copy(True)\n",
    "    train_index = [j for j in range(1,11) if j not in [i,val_index]]\n",
    "    train_objs = [v for k,v in drug_group_dict.items() if k in train_index]\n",
    "    train_objs_list = []\n",
    "    for train_obj in train_objs:\n",
    "        train_objs_list.extend(train_obj)\n",
    "    trainset = data_df[data_df[select_name].isin(train_objs_list)].copy(True)\n",
    "    trainset.to_csv(os.path.join(save_path,f'{clusterway}{i}{split_type}_Training.csv'),index=False)\n",
    "    valset.to_csv(os.path.join(save_path,f'{clusterway}{i}{split_type}_Validation.csv'),index=False)\n",
    "    testset.to_csv(os.path.join(save_path,f'{clusterway}{i}{split_type}_Test.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning based on cell line similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering based on cell line similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_distances(fps):\n",
    "    dist_matrix = []\n",
    "    N = len(fps)\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            dist = cosine(fps[i], fps[j])  \n",
    "            dist_matrix.append(dist)  \n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(dataset_path)\n",
    "cell_info = pd.read_csv(cell_info_path,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cells = list(set(data_df['depmap_id']))\n",
    "use_cell_exp = cell_info.loc[use_cells].copy(deep=True)\n",
    "\n",
    "cell_dict = dict()  \n",
    "for index in use_cell_exp.index:\n",
    "    exp_values = use_cell_exp.loc[index].values\n",
    "    cell_dict[index] = exp_values\n",
    "\n",
    "cell_list = list(cell_dict.values())\n",
    "cell_ids = list(cell_dict.keys())\n",
    "cell_exps = list(cell_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = calculate_cosine_distances(cell_exps)\n",
    "\n",
    "num_groups = 10  \n",
    "threshold = 0.03  \n",
    "\n",
    "raw_clusters = list(Butina.ClusterData(dist_matrix , len(cell_list),distThresh=threshold, isDistData=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def adjust_cell_clusters_balanced(clusters, num_samples, num_groups, random_seed=None):\n",
    "    random.seed(random_seed)\n",
    "    target_size = num_samples // num_groups\n",
    "\n",
    "    final_clusters = [[] for _ in range(num_groups)]\n",
    "    cluster_sizes = [0] * num_groups  \n",
    "\n",
    "    clusters.sort(key=len)\n",
    "\n",
    "    for cluster in clusters:\n",
    "        available_buckets = [i for i in range(num_groups) if cluster_sizes[i] + len(cluster) <= target_size]\n",
    "\n",
    "        if available_buckets:\n",
    "            if random_seed is not None:\n",
    "                chosen_bucket = random.choice(available_buckets)\n",
    "            else:\n",
    "                chosen_bucket = min(available_buckets, key=lambda i: cluster_sizes[i])\n",
    "        else:\n",
    "            chosen_bucket = cluster_sizes.index(min(cluster_sizes))\n",
    "\n",
    "        final_clusters[chosen_bucket].extend(cluster)\n",
    "        cluster_sizes[chosen_bucket] += len(cluster)\n",
    "\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_random_seed = 42\n",
    "clusters = adjust_cell_clusters_balanced(raw_clusters ,len(cell_list), num_groups,random_seed=cell_random_seed) \n",
    "\n",
    "cell_df = pd.DataFrame({'cell_id': cell_ids})\n",
    "for i,cluster in enumerate(clusters):\n",
    "    cell_df.loc[cluster, 'group'] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_df.to_csv(f'./data/ProcessedFile/data_split/{cell_random_seed}_clustered_cell_groups.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset partitioning based on cell line grouping information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_group_info = cell_df\n",
    "\n",
    "cell_group_dict = {}\n",
    "for i in range(1,11):\n",
    "    cgroup_df = cell_group_info[cell_group_info['group']==i]\n",
    "    cell_group_dict[i]= cgroup_df['cell_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_name = \"depmap_id\"\n",
    "split_type=\"cell_sim_blind\"\n",
    "clusterway =cell_random_seed\n",
    "\n",
    "save_path = f'./data/ProcessedFile/data_split'\n",
    "for i in range(1,11):\n",
    "    testset = data_df[data_df[select_name].isin(cell_group_dict[i])].copy(True)\n",
    "\n",
    "    val_index = i+1 if i<10 else 1  \n",
    "    valset = data_df[data_df[select_name].isin(cell_group_dict[val_index])].copy(True)\n",
    "\n",
    "    train_index = [j for j in range(1,11) if j not in [i,val_index]]\n",
    "    train_objs = [v for k,v in cell_group_dict.items() if k in train_index]\n",
    "    train_objs_list = []\n",
    "    for train_obj in train_objs:\n",
    "        train_objs_list.extend(train_obj)\n",
    "    trainset = data_df[data_df[select_name].isin(train_objs_list)].copy(True)\n",
    "\n",
    "    trainset.to_csv(os.path.join(save_path,f'{clusterway}{i}{split_type}_Training.csv'),index=False)\n",
    "    valset.to_csv(os.path.join(save_path,f'{clusterway}{i}{split_type}_Validation.csv'),index=False)\n",
    "    testset.to_csv(os.path.join(save_path,f'{clusterway}{i}{split_type}_Test.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioGDR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
